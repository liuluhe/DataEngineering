{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取位于S3中的视频数据，有两种方式读：glue表和文件形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource0 = glueContext.create_dynamic_frame.from_catalog(database = \"livecodingdemo\", table_name = \"raw_video_stats\", transformation_ctx = \"datasource0\")\n",
    "df_owner = spark.read.json('s3://aws-demo-live-code-demo/data/owner-stats/')\n",
    "df_video = datasource0.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_video.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# owner表中包括每小时爬取的数据，提取最新的数据做成当前快照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "rank_owner =  df_owner.withColumn(\"rank\", row_number().over(Window.partitionBy(\"mid\").orderBy(desc(\"crawl_date\"))))\n",
    "filter_owner = rank_owner.filter(rank_owner.rank == 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其中tag字段是一个Array结构的，可读性不够，需要二次处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df_flat = filter_owner.select('mid', 'follower','video_counts','crawl_date', explode('tag').alias('flat'))\n",
    "df = df_flat.select('mid', 'follower','video_counts','crawl_date', df_flat['flat'].getItem(\"游戏\").alias('tag:game'),df_flat['flat'].getItem(\"生活\").alias('tag:life'),df_flat['flat'].getItem(\"知识\").alias('tag:knowledge'),df_flat['flat'].getItem(\"美食\").alias('tag:food'),df_flat['flat'].getItem(\"音乐\").alias('tag:music'),df_flat['flat'].getItem(\"纪录片\").alias('tag:documentary'))\n",
    "df_final = df.groupBy('mid', 'follower','video_counts','crawl_date').agg(max('tag:game').alias('tag:game'),max('tag:life').alias('tag:life'),max('tag:knowledge').alias('tag:knowledge'),max('tag:documentary').alias('tag:documentary'),max('tag:food').alias('tag:food'),max('tag:music').alias('tag:mucis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把处理结果复写回S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write.format('parquet').mode(\"overwrite\").save(\"s3://aws-demo-live-code-demo/data/owner-stats-latest/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据格式和结构转化，提高查询效率\n",
    "## 方法一：通过时间戳判断新增数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "y = time.localtime().tm_year\n",
    "m = time.localtime().tm_mon\n",
    "d = time.localtime().tm_mday -1\n",
    "df_delta = df_owner.filter((df_owner.year==y)&(df_owner.month==m)&(df_owner.day==d))\n",
    "df_owner_final = df_delta.select('crawl_date','follower','following','mid','video_counts').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner_final.write.format('parquet').mode(\"append\").partitionBy('mid').save(\"s3://aws-demo-live-code-demo/data/owner-stats-clean/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法二：通过表的bookmark判断新增数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolvechoice2 = datasource0.resolveChoice(specs = [('mid','cast:string')])\n",
    "\n",
    "glueContext.write_dynamic_frame.from_options(frame = resolvechoice2, connection_type = \"s3\", connection_options = {\"path\": \"s3://aws-demo-live-code-demo/data/video-stats-clean/\",\"partitionKeys\": [\"mid\",\"year\"]}, format = \"parquet\", transformation_ctx = \"datasink0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource0.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolvechoice2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_owner.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
